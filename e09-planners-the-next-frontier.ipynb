{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E9 Planners and their future promise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install semantic-kernel==0.3.8.dev0\n",
    "!python -m pip install chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan our path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': <semantic_kernel.orchestration.sk_function.SKFunction at 0x1207785b0>,\n",
       " 'save': <semantic_kernel.orchestration.sk_function.SKFunction at 0x120778d30>}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion, OpenAITextEmbedding\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, AzureTextEmbedding\n",
    "from semantic_kernel.connectors.memory.chroma import ChromaMemoryStore\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "useAzureOpenAI = False\n",
    "\n",
    "# Configure AI services used by the kernel\n",
    "if useAzureOpenAI:\n",
    "    deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "    kernel.add_text_completion_service(\"dv\", AzureChatCompletion(deployment, endpoint, api_key))\n",
    "    kernel.add_text_embedding_generation_service(\"ada\", AzureTextEmbedding(\"text-embedding-ada-002\", api_key, endpoint))\n",
    "else:\n",
    "    api_key, org_id = sk.openai_settings_from_dot_env()\n",
    "    kernel.add_text_completion_service(\"dv\", OpenAIChatCompletion(\"gpt-4\", api_key, org_id))\n",
    "#    kernel.add_text_completion_service(\"dv\", OpenAIChatCompletion(\"gpt-3.5-turbo-0301\", api_key, org_id))\n",
    "    kernel.add_text_embedding_generation_service(\"ada\", OpenAITextEmbedding(\"text-embedding-ada-002\", api_key, org_id))\n",
    "\n",
    "kernel.register_memory_store(memory_store=ChromaMemoryStore(persist_directory='mymemories'))\n",
    "kernel.import_skill(sk.core_skills.TextMemorySkill())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.planning import SequentialPlanner\n",
    "from semantic_kernel.core_skills.text_skill import TextSkill\n",
    "\n",
    "skills_directory = \"./plugins-sk\"\n",
    "consultant_plugin = kernel.import_semantic_skill_from_directory(skills_directory, \"FriendlyConsultant\")\n",
    "\n",
    "# create an instance of sequential planner.\n",
    "planner = SequentialPlanner(kernel)\n",
    "\n",
    "# the ask for which the sequential planner is going to find a relevant function.\n",
    "# ask = \"\"\"\n",
    "# Create a business presentation about a hamburger restaurant.\"\"\"\n",
    "# ask = \"\"\"\n",
    "# Create a research interview about a hamburger restaurant business. The focus is on its strengths.\"\"\"\n",
    "# ask = \"\"\"\n",
    "# Create a business presentation about a hamburger restaurant business. The context is that has served the local community of Chicago for 43 years and is famous for chili burgers.\"\"\"\n",
    "ask = \"\"\"\n",
    "Create a marketing plan for a burger restaurant that has served the local community of Chicago for 43 years and is famous for chili burgers.\"\"\"\n",
    "\n",
    "# ask the sequential planner to identify a suitable function from the list of functions available.\n",
    "plan = await planner.create_plan_async(goal=ask)\n",
    "\n",
    "# ask the sequential planner to execute the identified function.\n",
    "result = await plan.invoke_async()\n",
    "\n",
    "for step in plan._steps:\n",
    "    print(\"PLAN \",step.description, \":\", step.skill_name + \".\" + step._function.name, step._parameters._variables, step._outputs)\n",
    "#print(\"LENGTH: \"+len(plan._steps))\n",
    "\n",
    "for index, step in enumerate(plan._steps):\n",
    "    print(\"Step:\", index)\n",
    "    print(\"Description:\",step.description)\n",
    "    print(\"Function:\", step.skill_name + \".\" + step._function.name)\n",
    "    print(\"Input vars:\", step._parameters._variables)\n",
    "    print(\"Output vars:\", step._outputs)\n",
    "    if len(step._outputs) > 0:\n",
    "        print( \"  Output:\", str.replace(result[step._outputs[0]],\"\\n\", \"\\n        \"))\n",
    "\n",
    "    print()\n",
    "# print(\"Final Answer:\")\n",
    "# print(result)\n",
    "\n",
    "# #for step in plan._steps:\n",
    "# #    print(step.description, \":\", step.skill_name, step._function.name, step._state.__dict__)\n",
    "\n",
    "# print(\"Expected Answer:\")\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion, OpenAITextEmbedding\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, AzureTextEmbedding\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "useAzureOpenAI = False\n",
    "\n",
    "# Configure AI services used by the kernel\n",
    "if useAzureOpenAI:\n",
    "    deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "    kernel.add_text_completion_service(\"dv\", AzureChatCompletion(deployment, endpoint, api_key))\n",
    "else:\n",
    "    api_key, org_id = sk.openai_settings_from_dot_env()\n",
    "#    kernel.add_text_completion_service(\"dv\", OpenAIChatCompletion(\"gpt-3.5-turbo\", api_key, org_id))\n",
    "    kernel.add_text_completion_service(\"dv\", OpenAIChatCompletion(\"gpt-3.5-turbo-0301\", api_key, org_id))\n",
    "#    kernel.add_text_completion_service(\"dv\", OpenAIChatCompletion(\"gpt-4\", api_key, org_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What day of the week is today, all uppercase?\n",
      "Step: 0\n",
      "Description: Get the current date.\n",
      "Function: time.today\n",
      "Input vars: {'input': ''}\n",
      "Output vars: ['TODAY']\n",
      "  Output: Monday, 14 August, 2023\n",
      "\n",
      "Step: 1\n",
      "Description: Get the current day of the week\n",
      "Function: time.dayOfWeek\n",
      "Input vars: {'input': '$TODAY'}\n",
      "Output vars: ['DAY_OF_WEEK']\n",
      "  Output: Monday\n",
      "\n",
      "Step: 2\n",
      "Description: Convert a string to uppercase.\n",
      "Function: text.uppercase\n",
      "Input vars: {'input': '$DAY_OF_WEEK'}\n",
      "Output vars: ['RESULT__DAY_OF_WEEK_UPPERCASE']\n",
      "  Output: MONDAY\n",
      "\n",
      "Get the current date. : time today {'_variables': {'input': ''}, '_main_key': 'input'}\n",
      "Get the current day of the week : time dayOfWeek {'_variables': {'input': ''}, '_main_key': 'input'}\n",
      "Convert a string to uppercase. : text uppercase {'_variables': {'input': ''}, '_main_key': 'input'}\n",
      "Expected Answer:\n",
      "MONDAY\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel.core_skills import FileIOSkill, MathSkill, TextSkill, TimeSkill\n",
    "from semantic_kernel.planning import SequentialPlanner\n",
    "\n",
    "kernel.import_skill(MathSkill(), \"math\")\n",
    "kernel.import_skill(FileIOSkill(), \"fileIO\")\n",
    "kernel.import_skill(TimeSkill(), \"time\")\n",
    "kernel.import_skill(TextSkill(), \"text\")\n",
    "\n",
    "# create an instance of sequential planner.\n",
    "planner = SequentialPlanner(kernel)\n",
    "\n",
    "# the ask for which the sequential planner is going to find a relevant function.\n",
    "ask = \"What day of the week is today, all uppercase?\"\n",
    "\n",
    "# ask the sequential planner to identify a suitable function from the list of functions available.\n",
    "plan = await planner.create_plan_async(goal=ask)\n",
    "\n",
    "print(plan.description)\n",
    "\n",
    "# ask the sequential planner to execute the identified function.\n",
    "result = await plan.invoke_async()\n",
    "\n",
    "for index, step in enumerate(plan._steps):\n",
    "    print(\"Step:\", index)\n",
    "    print(\"Description:\",step.description)\n",
    "    print(\"Function:\", step.skill_name + \".\" + step._function.name)\n",
    "    print(\"Input vars:\", step._parameters._variables)\n",
    "    print(\"Output vars:\", step._outputs)\n",
    "    if len(step._outputs) > 0:\n",
    "        print( \"  Output:\", str.replace(result[step._outputs[0]],\"\\n\", \"\\n        \"))\n",
    "\n",
    "    print()\n",
    "\n",
    "for step in plan._steps:\n",
    "    print(step.description, \":\", step.skill_name, step._function.name, step._state.__dict__)\n",
    "\n",
    "print(\"Expected Answer:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing the pizza shop owner is figure out how to get more time back for themself. Often times that time gained can be used for the pizza shop to reinvest in the same way that money saved can be re-leveraged."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
